{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Exercise for  FPM Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Exploring properties of the dataset accidents_10k.dat. Read more about it here:  http://fimi.uantwerpen.be/data/accidents.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 \n",
      "2 5 7 8 9 10 12 13 14 15 16 17 18 20 22 23 24 25 27 28 29 32 33 34 35 36 37 38 39 \n",
      "7 10 12 13 14 15 16 17 18 20 25 28 29 30 33 40 41 42 43 44 45 46 47 48 49 50 51 52 \n",
      "1 5 8 10 12 14 15 16 17 18 19 20 21 22 24 25 26 27 28 29 30 31 41 43 46 48 49 51 52 53 54 55 56 57 58 59 60 61 \n",
      "5 8 10 12 14 15 16 17 18 21 22 24 25 26 27 28 29 31 33 36 38 39 41 43 46 56 62 63 64 65 66 67 68 \n",
      "7 8 10 12 17 18 21 23 24 26 27 28 29 30 33 34 35 36 38 41 43 47 59 63 66 69 70 71 72 73 74 75 76 77 78 79 \n",
      "1 12 14 15 16 17 18 21 22 23 24 25 27 28 29 30 31 35 38 41 43 44 53 56 57 58 59 60 63 66 80 81 82 83 84 \n",
      "10 12 14 15 16 17 18 21 22 24 25 26 27 28 29 30 31 33 39 41 43 44 46 49 59 60 62 63 66 82 \n",
      "1 8 10 12 14 15 16 17 18 21 22 23 24 25 27 29 30 31 38 41 43 53 56 59 61 63 66 68 85 86 87 88 89 \n",
      "1 8 12 13 14 15 16 17 18 22 24 25 28 30 38 41 42 43 46 49 60 63 64 66 80 82 84 90 91 92 93 94 95 \n"
     ]
    }
   ],
   "source": [
    "!head accidents_10k.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 1a:** </span>. How many items are there in the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310\n"
     ]
    }
   ],
   "source": [
    "!awk -- '{for (i = 1; i <= NF; i++) wc[$i] += 1}; END {print length(wc)}' accidents_10k.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Answer:** </span> 310"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 1b:** </span> How many transactions are present in the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 accidents_10k.dat\n"
     ]
    }
   ],
   "source": [
    "!wc -l accidents_10k.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Answer:** </span> 10,000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 1c:** </span>.  What is the length of the smallest transaction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('accidents_10k.dat', names=['set'])\n",
    "lengths = df['set'].str.split(' ').apply(len)\n",
    "\n",
    "lengths.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Answer:** </span> 24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 1d:** </span>  What is the length of the longest transaction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Answer:** </span> 46"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 1e:** </span>  What is the size of the search space of frequent itemsets in this data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "311"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items = {}\n",
    "for set in df['set']:\n",
    "    for item in set.split(' '):\n",
    "        items[item] = 1\n",
    "        \n",
    "len(items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Answer:** </span> a set with n elements has 2^n subsets, so the search space is of size 2^311, which is very large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 1f:** </span> \n",
    "Assume that you work for the deparment of transportation that collected this data. What benefit do you see in using itemset mining approaches on this data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Answer:** </span> it can be used to see if there are correlations between details of the accidents, like whether \"poor weather conditions\" and \"at an intersection\" occur together frequently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 1g:** </span>  What type of itemsets (frequent, maximial or closed) would you be interested in discovering this dataset? State your reason."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Answer:** </span> I would be most interested in frequent itemsets for the reason stated above, it can be used to determine if there are particularly unsafe combinations of factors that result in accidents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 1h:** </span>  What minsup threshold would you use and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Answer:** </span> The size of the dataset is 10,000, which is not insignificant. a minsup threshold of 1000 would look for combinations of factors that appear in 10% of accidents - i think this is a reasonable starting point, and it can be adjusted from there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Generating frequent, maximal and closed itemsets using $\\color{red}{\\text{Apriori}}$, $\\color{red}{\\text{ECLAT}}$, and $\\color{red}{\\text{FPGrowth}}$ algorihtms from the dataset accidents_10k.dat "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 2a:** </span> Generate frequent itemsets using Apriori, for minsup = 2000, 3000, and 4000. Which of these minsup thresholds results in a maximum number of frequent itemsets? Which of these minsup thresholds results in a least number of frequent itemsets? Provide a rationale for these observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./apriori - find frequent item sets with the apriori algorithm\n",
      "version 6.27 (2017.08.01)        (c) 1996-2017   Christian Borgelt\n",
      "reading accidents_10k.dat ... [310 item(s), 10000 transaction(s)] done [0.04s].\n",
      "filtering, sorting and recoding items ... [49 item(s)] done [0.00s].\n",
      "sorting and reducing transactions ... [9951/10000 transaction(s)] done [0.01s].\n",
      "building transaction tree ... [20250 node(s)] done [0.00s].\n",
      "checking subsets of size 1 2 3 4 5 6 7 8 9 10 11 12 13 done [17.21s].\n",
      "writing accidents_ap_2000.txt ... [851034 set(s)] done [0.09s].\n",
      "./apriori - find frequent item sets with the apriori algorithm\n",
      "version 6.27 (2017.08.01)        (c) 1996-2017   Christian Borgelt\n",
      "reading accidents_10k.dat ... [310 item(s), 10000 transaction(s)] done [0.04s].\n",
      "filtering, sorting and recoding items ... [38 item(s)] done [0.00s].\n",
      "sorting and reducing transactions ... [9674/10000 transaction(s)] done [0.01s].\n",
      "building transaction tree ... [24741 node(s)] done [0.00s].\n",
      "checking subsets of size 1 2 3 4 5 6 7 8 9 10 11 12 done [3.93s].\n",
      "writing accidents_ap_3000.txt ... [133799 set(s)] done [0.04s].\n",
      "./apriori - find frequent item sets with the apriori algorithm\n",
      "version 6.27 (2017.08.01)        (c) 1996-2017   Christian Borgelt\n",
      "reading accidents_10k.dat ... [310 item(s), 10000 transaction(s)] done [0.02s].\n",
      "filtering, sorting and recoding items ... [33 item(s)] done [0.01s].\n",
      "sorting and reducing transactions ... [9381/10000 transaction(s)] done [0.00s].\n",
      "building transaction tree ... [22267 node(s)] done [0.01s].\n",
      "checking subsets of size 1 2 3 4 5 6 7 8 9 10 11 done [1.27s].\n",
      "writing accidents_ap_4000.txt ... [29501 set(s)] done [0.01s].\n"
     ]
    }
   ],
   "source": [
    "!chmod u+x apriori\n",
    "!./apriori -ts -s-2000 accidents_10k.dat accidents_ap_2000.txt\n",
    "!./apriori -ts -s-3000 accidents_10k.dat accidents_ap_3000.txt\n",
    "!./apriori -ts -s-4000 accidents_10k.dat accidents_ap_4000.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "851034 accidents_ap_2000.txt\n",
      "133799 accidents_ap_3000.txt\n",
      "29501 accidents_ap_4000.txt\n"
     ]
    }
   ],
   "source": [
    "!wc -l accidents_ap_2000.txt\n",
    "!wc -l accidents_ap_3000.txt\n",
    "!wc -l accidents_ap_4000.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Answer:** </span> 4000 minsup has the least with only 29501 frequent itemsets. this seems like a simple and fundemental concept and i am not sure how to really explain it. if you increase the requirements to obtain a result, you will obtain that result less frequently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 2b:** </span>   Using Apriori, compare the execution time for finding frequent itemsets for minsup = 2000, 3000, and 4000. Which of these minsup thresholds takes the least amount of time? Provide a rationale for this observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Answer:** </span> As per the times shown above, A threshold of 4000 takes the shortest time, 1.2 seconds compared to 17 seconds for a threshold of 2000. I am pretty sure this is because once a subset is deemed infrequent, you can stop searching beyond that subset / cut a branch off a tree, and there is less to search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 2c:** </span> Using Apriori, find the frequent itemsets for minsup = 2000, 3000, and 4000. Determine the number of itemsets for each size (1 to max length of an itemset). What trends do you see that are common for all three minsup thresholds? What trends do you see that are different? Provide a rationale for these observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./apriori - find frequent item sets with the apriori algorithm\n",
      "version 6.27 (2017.08.01)        (c) 1996-2017   Christian Borgelt\n",
      "reading accidents_10k.dat ... [310 item(s), 10000 transaction(s)] done [0.04s].\n",
      "filtering, sorting and recoding items ... [49 item(s)] done [0.00s].\n",
      "sorting and reducing transactions ... [9951/10000 transaction(s)] done [0.01s].\n",
      "building transaction tree ... [20250 node(s)] done [0.00s].\n",
      "checking subsets of size 1 2 3 4 5 6 7 8 9 10 11 12 13 done [16.21s].\n",
      "writing accidents_ap_2000.txt ... [851034 set(s)] done [0.09s].\n",
      "all: 851034\n",
      "  0: 0\n",
      "  1: 49\n",
      "  2: 705\n",
      "  3: 5285\n",
      "  4: 23745\n",
      "  5: 69647\n",
      "  6: 139628\n",
      "  7: 195730\n",
      "  8: 193299\n",
      "  9: 133819\n",
      " 10: 63937\n",
      " 11: 20497\n",
      " 12: 4189\n",
      " 13: 483\n",
      " 14: 21\n",
      "./apriori - find frequent item sets with the apriori algorithm\n",
      "version 6.27 (2017.08.01)        (c) 1996-2017   Christian Borgelt\n",
      "reading accidents_10k.dat ... [310 item(s), 10000 transaction(s)] done [0.04s].\n",
      "filtering, sorting and recoding items ... [38 item(s)] done [0.00s].\n",
      "sorting and reducing transactions ... [9674/10000 transaction(s)] done [0.01s].\n",
      "building transaction tree ... [24741 node(s)] done [0.02s].\n",
      "checking subsets of size 1 2 3 4 5 6 7 8 9 10 11 12 done [3.92s].\n",
      "writing accidents_ap_3000.txt ... [133799 set(s)] done [0.02s].\n",
      "all: 133799\n",
      "  0: 0\n",
      "  1: 38\n",
      "  2: 468\n",
      "  3: 2830\n",
      "  4: 9887\n",
      "  5: 21779\n",
      "  6: 31964\n",
      "  7: 32020\n",
      "  8: 21862\n",
      "  9: 9839\n",
      " 10: 2705\n",
      " 11: 387\n",
      " 12: 20\n",
      "./apriori - find frequent item sets with the apriori algorithm\n",
      "version 6.27 (2017.08.01)        (c) 1996-2017   Christian Borgelt\n",
      "reading accidents_10k.dat ... [310 item(s), 10000 transaction(s)] done [0.02s].\n",
      "filtering, sorting and recoding items ... [33 item(s)] done [0.01s].\n",
      "sorting and reducing transactions ... [9381/10000 transaction(s)] done [0.01s].\n",
      "building transaction tree ... [22267 node(s)] done [0.00s].\n",
      "checking subsets of size 1 2 3 4 5 6 7 8 9 10 11 done [0.90s].\n",
      "writing accidents_ap_4000.txt ... [29501 set(s)] done [0.00s].\n",
      "all: 29501\n",
      "  0: 0\n",
      "  1: 33\n",
      "  2: 319\n",
      "  3: 1492\n",
      "  4: 4043\n",
      "  5: 6926\n",
      "  6: 7751\n",
      "  7: 5626\n",
      "  8: 2546\n",
      "  9: 668\n",
      " 10: 91\n",
      " 11: 6\n"
     ]
    }
   ],
   "source": [
    "!chmod u+x apriori\n",
    "!./apriori -ts -Z -s-2000 accidents_10k.dat accidents_ap_2000.txt\n",
    "!./apriori -ts -Z -s-3000 accidents_10k.dat accidents_ap_3000.txt\n",
    "!./apriori -ts -Z -s-4000 accidents_10k.dat accidents_ap_4000.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Answer:** </span> Number of itemsets of each size is shown in the output above. A trend i see is that as you increase the minsup threshold, the largest sized frequent itemset gets smaller. For 2000 it is 14, for 3000 it is 12, for 2000 it is 11. This makes sense because the larger the set, the less likely it is to occur. So a lower threshold means it is the threshold for a larger set to be frequent becomes more likely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 2d:** </span>  Using Apriori with minsup=2000, compare the number of frequent, maximal, and closed itemsets. Which is the largest set and which is the smallest set? Provide a rationale for these observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./apriori - find frequent item sets with the apriori algorithm\n",
      "version 6.27 (2017.08.01)        (c) 1996-2017   Christian Borgelt\n",
      "reading accidents_10k.dat ... [310 item(s), 10000 transaction(s)] done [0.04s].\n",
      "filtering, sorting and recoding items ... [49 item(s)] done [0.00s].\n",
      "sorting and reducing transactions ... [9951/10000 transaction(s)] done [0.01s].\n",
      "building transaction tree ... [20250 node(s)] done [0.01s].\n",
      "checking subsets of size 1 2 3 4 5 6 7 8 9 10 11 12 13 done [15.51s].\n",
      "writing accidents_ap_2000.txt ... [851034 set(s)] done [0.08s].\n",
      "all: 851034\n",
      "  0: 0\n",
      "  1: 49\n",
      "  2: 705\n",
      "  3: 5285\n",
      "  4: 23745\n",
      "  5: 69647\n",
      "  6: 139628\n",
      "  7: 195730\n",
      "  8: 193299\n",
      "  9: 133819\n",
      " 10: 63937\n",
      " 11: 20497\n",
      " 12: 4189\n",
      " 13: 483\n",
      " 14: 21\n",
      "./apriori - find frequent item sets with the apriori algorithm\n",
      "version 6.27 (2017.08.01)        (c) 1996-2017   Christian Borgelt\n",
      "reading accidents_10k.dat ... [310 item(s), 10000 transaction(s)] done [0.04s].\n",
      "filtering, sorting and recoding items ... [49 item(s)] done [0.00s].\n",
      "sorting and reducing transactions ... [9951/10000 transaction(s)] done [0.01s].\n",
      "building transaction tree ... [20250 node(s)] done [0.01s].\n",
      "checking subsets of size 1 2 3 4 5 6 7 8 9 10 11 12 13 14 done [22.35s].\n",
      "filtering for closed item sets ... done [0.33s].\n",
      "writing accidents_ap_2000.txt ... [519902 set(s)] done [0.08s].\n",
      "all: 519902\n",
      "  0: 0\n",
      "  1: 29\n",
      "  2: 370\n",
      "  3: 2702\n",
      "  4: 12573\n",
      "  5: 38222\n",
      "  6: 79042\n",
      "  7: 114992\n",
      "  8: 119533\n",
      "  9: 88250\n",
      " 10: 45043\n",
      " 11: 15322\n",
      " 12: 3368\n",
      " 13: 435\n",
      " 14: 21\n",
      "./apriori - find frequent item sets with the apriori algorithm\n",
      "version 6.27 (2017.08.01)        (c) 1996-2017   Christian Borgelt\n",
      "reading accidents_10k.dat ... [310 item(s), 10000 transaction(s)] done [0.04s].\n",
      "filtering, sorting and recoding items ... [49 item(s)] done [0.01s].\n",
      "sorting and reducing transactions ... [9951/10000 transaction(s)] done [0.01s].\n",
      "building transaction tree ... [20250 node(s)] done [0.00s].\n",
      "checking subsets of size 1 2 3 4 5 6 7 8 9 10 11 12 13 14 done [23.07s].\n",
      "filtering for maximal item sets ... done [0.03s].\n",
      "writing accidents_ap_2000.txt ... [12330 set(s)] done [0.01s].\n",
      "all: 12330\n",
      "  0: 0\n",
      "  1: 0\n",
      "  2: 1\n",
      "  3: 1\n",
      "  4: 13\n",
      "  5: 44\n",
      "  6: 242\n",
      "  7: 718\n",
      "  8: 1522\n",
      "  9: 2606\n",
      " 10: 3318\n",
      " 11: 2598\n",
      " 12: 993\n",
      " 13: 253\n",
      " 14: 21\n"
     ]
    }
   ],
   "source": [
    "!./apriori -ts -Z -s-2000 accidents_10k.dat accidents_ap_2000.txt\n",
    "!./apriori -tc -Z -s-2000 accidents_10k.dat accidents_ap_2000.txt\n",
    "!./apriori -tm -Z -s-2000 accidents_10k.dat accidents_ap_2000.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Answer:** </span> The smallest frequent and maximal itemset is 1, and the smallest closed itemset is 2. I believe it is not the same as the smallest closed itemset because a closed itemset cannot be a subset of another frequent itemset further down the tree, so it is harder to have very small sets. in all cases, the size of the largest set is the same, which makes sense because the definition of closed/frequent/maximal itemset does not really have much of an impact on the largest frequent sets.\n",
    "\n",
    "Unless you mean which group has the most sets in it, in which case the smallest is closed, largest is frequent. this is because maximal sets are defined to be subsets of frequent sets, and closed sets are defined to be subsets of maximal sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 2e:** </span> For a minsup = 2000, compare the execution time for Apriori, ECLAT and FPGrowth. Which of these algorithms took the least amount of time. Provide a rationale for this observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./apriori - find frequent item sets with the apriori algorithm\n",
      "version 6.27 (2017.08.01)        (c) 1996-2017   Christian Borgelt\n",
      "reading accidents_10k.dat ... [310 item(s), 10000 transaction(s)] done [0.04s].\n",
      "filtering, sorting and recoding items ... [49 item(s)] done [0.01s].\n",
      "sorting and reducing transactions ... [9951/10000 transaction(s)] done [0.01s].\n",
      "building transaction tree ... [20250 node(s)] done [0.00s].\n",
      "checking subsets of size 1 2 3 4 5 6 7 8 9 10 11 12 13 done [15.41s].\n",
      "writing accidents_ap_2000.txt ... [851034 set(s)] done [0.08s].\n",
      "./eclat - find frequent item sets with the eclat algorithm\n",
      "version 5.20 (2017.05.30)        (c) 2002-2017   Christian Borgelt\n",
      "reading T10I4D100K_new.dat ... [870 item(s), 99936 transaction(s)] done [0.10s].\n",
      "filtering, sorting and recoding items ... [155 item(s)] done [0.01s].\n",
      "sorting and reducing transactions ... [75349/99936 transaction(s)] done [0.02s].\n",
      "writing accidents_ap_2000.txt ... [155 set(s)] done [0.03s].\n",
      "./fpgrowth - find frequent item sets with the fpgrowth algorithm\n",
      "version 6.17 (2017.05.30)        (c) 2004-2017   Christian Borgelt\n",
      "reading T10I4D100K_new.dat ... [870 item(s), 99936 transaction(s)] done [0.10s].\n",
      "filtering, sorting and recoding items ... [155 item(s)] done [0.01s].\n",
      "sorting and reducing transactions ... [75349/99936 transaction(s)] done [0.02s].\n",
      "writing accidents_ap_2000.txt ... [155 set(s)] done [0.03s].\n"
     ]
    }
   ],
   "source": [
    "!chmod u+x fpgrowth\n",
    "!chmod u+x eclat\n",
    "!./apriori -ts -s-2000 accidents_10k.dat accidents_ap_2000.txt\n",
    "!./eclat -ts -s-2000 T10I4D100K_new.dat accidents_ap_2000.txt\n",
    "!./fpgrowth -ts -s-2000 T10I4D100K_new.dat accidents_ap_2000.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Answer:** </span> It took apriori 15 seconds, but it only took th eother algorithms .02 seconds, i think? I suppose this is because you dont have to keep looking at the entire dataset as you progress, which makes it much faster. also, i believe it is faster to look at integer indices rather than strinigs. interestingly. it seems like eclat and fpgrowth took the same amount of time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 2f:** </span> For a minsup = 4000, compare the execution time for Apriori, ECLAT and FPGrowth. Which of these algorithms took the least amount of time. Provide a rationale for this observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./apriori - find frequent item sets with the apriori algorithm\n",
      "version 6.27 (2017.08.01)        (c) 1996-2017   Christian Borgelt\n",
      "reading accidents_10k.dat ... [310 item(s), 10000 transaction(s)] done [0.04s].\n",
      "filtering, sorting and recoding items ... [33 item(s)] done [0.00s].\n",
      "sorting and reducing transactions ... [9381/10000 transaction(s)] done [0.02s].\n",
      "building transaction tree ... [22267 node(s)] done [0.00s].\n",
      "checking subsets of size 1 2 3 4 5 6 7 8 9 10 11 done [1.16s].\n",
      "writing accidents_ap_2000.txt ... [29501 set(s)] done [0.00s].\n",
      "./eclat - find frequent item sets with the eclat algorithm\n",
      "version 5.20 (2017.05.30)        (c) 2002-2017   Christian Borgelt\n",
      "reading T10I4D100K_new.dat ... [870 item(s), 99936 transaction(s)] done [0.11s].\n",
      "filtering, sorting and recoding items ... [26 item(s)] done [0.00s].\n",
      "sorting and reducing transactions ... [5456/99936 transaction(s)] done [0.01s].\n",
      "writing accidents_ap_2000.txt ... [26 set(s)] done [0.00s].\n",
      "./fpgrowth - find frequent item sets with the fpgrowth algorithm\n",
      "version 6.17 (2017.05.30)        (c) 2004-2017   Christian Borgelt\n",
      "reading T10I4D100K_new.dat ... [870 item(s), 99936 transaction(s)] done [0.11s].\n",
      "filtering, sorting and recoding items ... [26 item(s)] done [0.01s].\n",
      "sorting and reducing transactions ... [5456/99936 transaction(s)] done [0.01s].\n",
      "writing accidents_ap_2000.txt ... [26 set(s)] done [0.00s].\n"
     ]
    }
   ],
   "source": [
    "!./apriori -ts -s-4000 accidents_10k.dat accidents_ap_2000.txt\n",
    "!./eclat -ts -s-4000 T10I4D100K_new.dat accidents_ap_2000.txt\n",
    "!./fpgrowth -ts -s-4000 T10I4D100K_new.dat accidents_ap_2000.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Answer:** </span> again, it appears that fpgrowth and eclat both took around 0.01 seconds. there does not seem to be any substantial change from the reasoning given in the previous question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 2g:** </span>  For a minsup = 6000, compare the execution time for Apriori, ECLAT and FPGrowth. Which of these algorithms took the least amount of time. Provide a rationale for this observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./apriori - find frequent item sets with the apriori algorithm\n",
      "version 6.27 (2017.08.01)        (c) 1996-2017   Christian Borgelt\n",
      "reading accidents_10k.dat ... [310 item(s), 10000 transaction(s)] done [0.04s].\n",
      "filtering, sorting and recoding items ... [20 item(s)] done [0.00s].\n",
      "sorting and reducing transactions ... [3216/10000 transaction(s)] done [0.01s].\n",
      "building transaction tree ... [6478 node(s)] done [0.00s].\n",
      "checking subsets of size 1 2 3 4 5 6 7 8 done [0.06s].\n",
      "writing accidents_ap_2000.txt ... [2254 set(s)] done [0.00s].\n",
      "./eclat - find frequent item sets with the eclat algorithm\n",
      "version 5.20 (2017.05.30)        (c) 2002-2017   Christian Borgelt\n",
      "reading T10I4D100K_new.dat ... [870 item(s), 99936 transaction(s)] done [0.11s].\n",
      "filtering, sorting and recoding items ... [4 item(s)] done [0.00s].\n",
      "sorting and reducing transactions ... [16/99936 transaction(s)] done [0.01s].\n",
      "writing accidents_ap_2000.txt ... [4 set(s)] done [0.00s].\n",
      "./fpgrowth - find frequent item sets with the fpgrowth algorithm\n",
      "version 6.17 (2017.05.30)        (c) 2004-2017   Christian Borgelt\n",
      "reading T10I4D100K_new.dat ... [870 item(s), 99936 transaction(s)] done [0.10s].\n",
      "filtering, sorting and recoding items ... [4 item(s)] done [0.01s].\n",
      "sorting and reducing transactions ... [16/99936 transaction(s)] done [0.00s].\n",
      "writing accidents_ap_2000.txt ... [4 set(s)] done [0.00s].\n"
     ]
    }
   ],
   "source": [
    "!./apriori -ts -s-6000 accidents_10k.dat accidents_ap_2000.txt\n",
    "!./eclat -ts -s-6000 T10I4D100K_new.dat accidents_ap_2000.txt\n",
    "!./fpgrowth -ts -s-6000 T10I4D100K_new.dat accidents_ap_2000.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Answer:** </span> this time, it looks like fpgrowth was faster than eclat, at 0.00 seconds vs 0.01 seconds. The only substantial difference i see in terms of efficiency between fpgrowth and eclat is that if most of your frequent sets are similar, the tree generated by the fpgrowth algorithm will not have many branches, and therefore be quicker. I am not sure if this is why it is faster in this particular case, although with a higher minsup threshold, i would expect a less complicated tree than in the previous question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 2h:** </span> Fill the following table based on execution times computed in __2e__, __2f__, and __2g__. State your observations on the relative computational efficiency at different support thresholds. Based on your knowledge of these algorithms, provide the reasons behind your observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|   Algorithm                |minsup=2000         |minsup=4000         |minsup=6000         |\n",
    "|----------------------------|--------------------|--------------------|--------------------|    \n",
    "|Apriori                     |15.41s              |1.16s               |0.06s               |\n",
    "|Eclat                       |0.10s               |0.01s               |0.01s               |\n",
    "|FPGrowth                    |0.10s               |0.01s               |0.00s               |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Answer:** </span> See answer to 2G and 2E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Discovering frequent subsequences and substrings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that roads in a Cincinnati are assigned numbers. Participants are enrolled in a transportation study and for every trip they make using their car, the sequence of roads taken are recorded. Trips that involves freeways are excluded. This data is in the file <span style=\"color:blue\">road_seq_data.dat</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 3a:** </span>  What 'type' of sequence mining will you perform to determine frequently taken 'paths'? Paths are sequences of roads traveresed consecutively in the same order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Answer:** </span> I would look at substrings instead of subsequences, since using subsequences would imply you can skip roads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 3b:** </span> How many sequences are there in this sequence database?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Answer:** </span> 1000 (see below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 3c:** </span> What is the size of the alphabet in this sequence database?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sequences:  1000\n",
      "alphabet:  1283\n"
     ]
    }
   ],
   "source": [
    "!chmod u+x seqwog\n",
    "df = pd.read_csv('road_seq_data.dat', names=['set'])\n",
    "sequences = df['set'].str.split(' ')\n",
    "print('number of sequences: ', len(sequences))\n",
    "vals = {}\n",
    "for list in sequences:\n",
    "    for string in list:\n",
    "        vals[string] = 1\n",
    "        \n",
    "print('alphabet: ', len(vals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Answer:** </span> 1283"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 3d:** </span> What are the total number of possible subsequences of length 2 in this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PrefixSpan version 1.00 - Sequential Pattern Miner\n",
      "Written by Yasuo Tabei\n",
      "\n",
      "242233 road_seq_data_minsup_2\n"
     ]
    }
   ],
   "source": [
    "!./prefixspan -min_sup 2 road_seq_data.dat | sed -n 'p;n'> road_seq_data_minsup_2\n",
    "!wc -l road_seq_data_minsup_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Answer:** </span> 242233 that are possible with this specific dataset. i do not know if you mean theoretically possible, which would be a different number, or possible with what is present in this dataset, in which case it would be this number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 3e:** </span> What are the total number of possible substrings of length 2 in this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./seqwog - find frequent sequences without gaps\n",
      "version 3.16 (2016.10.15)        (c) 2010-2016   Christian Borgelt\n",
      "reading road_seq_data.dat ... [1283 item(s), 1000 transaction(s)] done [0.01s].\n",
      "recoding items ... [1283 item(s)] done [0.00s].\n",
      "reducing and triming transactions ... [883/1000 transaction(s)] done [0.00s].\n",
      "writing substring_result ... [5968 sequence(s)] done [0.02s].\n",
      "5968 substring_result\n"
     ]
    }
   ],
   "source": [
    "!./seqwog -ts -s-2 road_seq_data.dat substring_result\n",
    "!wc -l substring_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Answer:** </span> 5968, maybe. Same confusion as above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 3f:** </span> Discover frequent __subsequences__ with minsup = 10 and report the number of subsequences discovered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PrefixSpan version 1.00 - Sequential Pattern Miner\n",
      "Written by Yasuo Tabei\n",
      "\n",
      "4589 road_seq_data_minsup_10\n"
     ]
    }
   ],
   "source": [
    "!chmod u+x prefixspan\n",
    "!./prefixspan -min_sup 10 road_seq_data.dat | sed -n 'p;n'> road_seq_data_minsup_10\n",
    "!wc -l road_seq_data_minsup_10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Answer:** </span> There are 4589 subsequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 3g:** </span>  Discover frequent __substrings__ with minsup = 10 and report the number of substrings discovered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./seqwog - find frequent sequences without gaps\n",
      "version 3.16 (2016.10.15)        (c) 2010-2016   Christian Borgelt\n",
      "reading road_seq_data.dat ... [1283 item(s), 1000 transaction(s)] done [0.01s].\n",
      "recoding items ... [1283 item(s)] done [0.00s].\n",
      "reducing and triming transactions ... [844/1000 transaction(s)] done [0.00s].\n",
      "writing substring_result ... [613 sequence(s)] done [0.00s].\n",
      "613 substring_result\n"
     ]
    }
   ],
   "source": [
    "!chmod u+x seqwog\n",
    "!./seqwog -ts -s-10 road_seq_data.dat substring_result\n",
    "!wc -l substring_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Answer:** </span> There are 613 substrings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 3h:** </span> Explain the difference in the number of frequent subsequences and substrings found in __3f__ and __3g__ above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Answer:** </span> subsequences do not have to take items from a sequence sequentially - they can take one element, skip the next, take the next, and so on. substrings must take consecutive elements to create the substring. this means that for a given sequence of length N, there will be far more subsequences than substrings, which we see above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-will-gpu] *",
   "language": "python",
   "name": "conda-env-.conda-will-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
